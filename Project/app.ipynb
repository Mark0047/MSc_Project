{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\anaconda3\\envs\\new_env_name\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Writing Documents: 10000it [00:00, 476138.49it/s]       \n",
      "c:\\Users\\haris\\anaconda3\\envs\\new_env_name\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Documents Processed: 10000 docs [00:00, 11289.06 docs/s]     \n",
      "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:   7%|▋         | 1/14 [00:06<01:26,  6.62s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  14%|█▍        | 2/14 [00:13<01:23,  6.97s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  21%|██▏       | 3/14 [00:21<01:18,  7.17s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  29%|██▊       | 4/14 [00:27<01:09,  6.91s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  36%|███▌      | 5/14 [00:34<01:01,  6.88s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  43%|████▎     | 6/14 [00:41<00:55,  6.93s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  50%|█████     | 7/14 [00:48<00:47,  6.85s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  57%|█████▋    | 8/14 [00:55<00:40,  6.82s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  64%|██████▍   | 9/14 [01:02<00:34,  6.95s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  71%|███████▏  | 10/14 [01:09<00:27,  6.90s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  79%|███████▊  | 11/14 [01:15<00:20,  6.77s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  86%|████████▌ | 12/14 [01:22<00:13,  6.73s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  93%|█████████▎| 13/14 [01:28<00:06,  6.72s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples: 100%|██████████| 14/14 [01:32<00:00,  6.64s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 63, Score: 0.9733178615570068\n",
      "Folder Context: health\n",
      "File Path: documents/health\n",
      "\n",
      "Answer: 10014460, Score: 0.9580560922622681\n",
      "Folder Context: health\n",
      "File Path: documents/health\n",
      "\n",
      "Answer: 10021739, Score: 0.9542986154556274\n",
      "Folder Context: health\n",
      "File Path: documents/health\n",
      "\n",
      "Answer 1: 63\n",
      "Context:      Private           5        Full inspection      04/08/2016     10015063      Requires improvement                                    NaN         \n",
      "Folder Context: health\n",
      "Score: 0.9733178615570068\n",
      "\n",
      "Answer 2: 10014460\n",
      "Context:         Private           6        Full inspection      19/05/2016     10014460      Requires improvement                                    NaN      \n",
      "Folder Context: health\n",
      "Score: 0.9580560922622681\n",
      "\n",
      "Answer 3: 10021739\n",
      "Context:         Private           4        Full inspection      07/09/2016     10021739      Requires improvement                                    NaN      \n",
      "Folder Context: health\n",
      "Score: 0.9542986154556274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.utils import clean_wiki_text\n",
    "import pandas as pd\n",
    "# import pdfplumber\n",
    "# import docx\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize FAISS document store\n",
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
    "\n",
    "# Helper function to read different file formats and handle nested folders\n",
    "def load_documents(dir_path=\"documents/\"):\n",
    "    docs = []\n",
    "    \n",
    "    for root, _, files in os.walk(dir_path):  # Recursively traverse directories\n",
    "        folder_context = os.path.basename(root)  # Capture folder name for context\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            ext = filename.split(\".\")[-1].lower()\n",
    "            text = \"\"\n",
    "            \n",
    "            try:\n",
    "                if ext == \"pdf\":\n",
    "                    with pdfplumber.open(file_path) as pdf:\n",
    "                        text = \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "                        \n",
    "                elif ext == \"docx\":\n",
    "                    doc = docx.Document(file_path)\n",
    "                    text = \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "                    \n",
    "                elif ext == \"csv\":\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    text = df.to_string(index=False)\n",
    "                    \n",
    "                elif ext == \"xlsx\":\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    text = df.to_string(index=False)\n",
    "                    \n",
    "                elif ext == \"json\":\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        json_data = json.load(json_file)\n",
    "                        text = json.dumps(json_data, indent=4)\n",
    "                        \n",
    "                elif ext == \"txt\":\n",
    "                    with open(file_path, \"r\") as text_file:\n",
    "                        text = text_file.read()\n",
    "                        \n",
    "                # Clean and add document with folder context\n",
    "                if text:\n",
    "                    docs.append({\n",
    "                        \"content\": clean_wiki_text(text),\n",
    "                        \"meta\": {\"name\": filename, \"folder_context\": folder_context, \"path\": root}\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "# Convert documents and write them to the document store\n",
    "docs = load_documents(\"documents/\")\n",
    "document_store.write_documents(docs)\n",
    "\n",
    "# Load retriever model and tokenizer\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True\n",
    ")\n",
    "\n",
    "# Update embeddings to enable fast retrieval\n",
    "document_store.update_embeddings(retriever)\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Initialize the FARMReader\n",
    "reader = FARMReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", use_gpu=True)\n",
    "\n",
    "# Build QA pipeline\n",
    "pipeline = ExtractiveQAPipeline(reader, retriever)\n",
    "\n",
    "# Function to answer queries\n",
    "def answer_query(query):\n",
    "    prediction = pipeline.run(query=query, params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 3}})\n",
    "    answers = prediction[\"answers\"]\n",
    "    for answer in answers:\n",
    "        folder_context = answer.meta.get(\"folder_context\", \"N/A\")\n",
    "        file_path = answer.meta.get(\"path\", \"Unknown path\")\n",
    "        print(f\"Answer: {answer.answer}, Score: {answer.score}\\nFolder Context: {folder_context}\\nFile Path: {file_path}\\n\")\n",
    "    return answers\n",
    "\n",
    "# Query Example\n",
    "query = \"How many children's social care Overall effectiveness Required improvement?\"\n",
    "answers = answer_query(query)\n",
    "\n",
    "for idx, answer in enumerate(answers):\n",
    "    print(f\"Answer {idx + 1}: {answer.answer}\\nContext: {answer.context}\\nFolder Context: {answer.meta.get('folder_context')}\\nScore: {answer.score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:   7%|▋         | 1/14 [00:06<01:21,  6.26s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  14%|█▍        | 2/14 [00:12<01:13,  6.16s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  21%|██▏       | 3/14 [00:19<01:10,  6.43s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  29%|██▊       | 4/14 [00:26<01:06,  6.64s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  36%|███▌      | 5/14 [00:32<00:59,  6.61s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  43%|████▎     | 6/14 [00:39<00:53,  6.64s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  50%|█████     | 7/14 [00:45<00:46,  6.59s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  57%|█████▋    | 8/14 [00:52<00:38,  6.49s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  64%|██████▍   | 9/14 [00:58<00:32,  6.41s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  71%|███████▏  | 10/14 [01:04<00:25,  6.36s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  79%|███████▊  | 11/14 [01:10<00:18,  6.28s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  86%|████████▌ | 12/14 [01:16<00:12,  6.23s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples:  93%|█████████▎| 13/14 [01:22<00:06,  6.18s/ Batches]'segment_ids' is not None, but DistilBert does not use them. They will be ignored.\n",
      "Inferencing Samples: 100%|██████████| 14/14 [01:26<00:00,  6.18s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 10014110, Score: 0.942974328994751\n",
      "Folder Context: health\n",
      "File Path: documents/health\n",
      "\n",
      "Answer: 10013541, Score: 0.9376291036605835\n",
      "Folder Context: health\n",
      "File Path: documents/health\n",
      "\n",
      "Answer: 10014322, Score: 0.8968779444694519\n",
      "Folder Context: health\n",
      "File Path: documents/health\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How many children's social care Overall effectiveness Required improvement? hint check Overall effectiveness column\"\n",
    "answers = answer_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Your custom training data\n",
    "    eval_dataset=eval_dataset     # Your custom evaluation data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/query/\")\n",
    "def get_answer(request: QueryRequest):\n",
    "    answers = answer_query(request.question)\n",
    "    return {\"answers\": [answer.answer for answer in answers]}\n",
    "\n",
    "# Run the app\n",
    "# uvicorn app:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 24.2 from c:\\Users\\haris\\anaconda3\\envs\\new_env_name\\lib\\site-packages\\pip (python 3.9)\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip -V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
